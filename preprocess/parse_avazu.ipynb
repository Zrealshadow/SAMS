{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangdan/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/huangdan/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"/hdd1/sams/avazu/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [10:06, 14.79s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000000 # 1,000,000\n",
    "cnt = 0\n",
    "col2value = {}\n",
    "for chunk in tqdm(pd.read_csv(train_file_path, chunksize=chunk_size, index_col='id', dtype=str)):\n",
    "    # Process the chunk if needed (e.g., data cleaning, manipulation)\n",
    "    for col in chunk.columns:\n",
    "        chunk[col].unique()\n",
    "        if col not in col2value:\n",
    "            col2value[col] = set()\n",
    "        unique_values = set(chunk[col].unique())\n",
    "        col2value[col] = col2value[col] | unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = chunk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column click has 2 unique key\n",
      "column hour has 240 unique key\n",
      "column C1 has 7 unique key\n",
      "column banner_pos has 7 unique key\n",
      "column site_id has 4737 unique key\n",
      "column site_domain has 7745 unique key\n",
      "column site_category has 26 unique key\n",
      "column app_id has 8552 unique key\n",
      "column app_domain has 559 unique key\n",
      "column app_category has 36 unique key\n",
      "column device_id has 2686408 unique key\n",
      "column device_ip has 6729486 unique key\n",
      "column device_model has 8251 unique key\n",
      "column device_type has 5 unique key\n",
      "column device_conn_type has 4 unique key\n",
      "column C14 has 2626 unique key\n",
      "column C15 has 8 unique key\n",
      "column C16 has 9 unique key\n",
      "column C17 has 435 unique key\n",
      "column C18 has 4 unique key\n",
      "column C19 has 68 unique key\n",
      "column C20 has 172 unique key\n",
      "column C21 has 60 unique key\n"
     ]
    }
   ],
   "source": [
    "for col in columns_list:\n",
    "    n = len(col2value[col])\n",
    "    print(f\"column {col} has {n} unique key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need id,\n",
    "tokenizer = {}\n",
    "# col_name: col_tokenizer ->\n",
    "# value : token_id\n",
    "cnt = 0\n",
    "for col, values in col2value.items():\n",
    "    if col in ['click']:\n",
    "        continue\n",
    "    tokenizer.setdefault(col, {})\n",
    "    for val in values:\n",
    "        val = val\n",
    "        tokenizer[col][val] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/hdd1/sams/avazu/origin_tokenizer.json\"\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [22:19, 32.66s/it]\n"
     ]
    }
   ],
   "source": [
    "array_list = []\n",
    "chunk_size = 1000000 # 1,000,000\n",
    "# --> 50,000,000\n",
    "for chunk in tqdm(pd.read_csv(train_file_path, chunksize=chunk_size, index_col='id', dtype=str)):\n",
    "    # Process the chunk if needed (e.g., data cleaning, manipulation)\n",
    "    chunk['click'] = chunk['click'].astype(int)\n",
    "    for col, col_tokenizer in tokenizer.items():\n",
    "        chunk[col] = chunk[col].map(col_tokenizer)\n",
    "    array_list.append(chunk.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.concatenate(array_list, axis=0)\n",
    "file_path = \"/hdd1/sams/avazu/data.npy\"\n",
    "np.save(file_path, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "data = feature\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "num_train = int(train_ratio * num_samples)\n",
    "num_val = int(val_ratio * num_samples)\n",
    "num_test = num_samples - num_train - num_val\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data\n",
    "train_data = data[:num_train]\n",
    "val_data = data[num_train:num_train + num_val]\n",
    "test_data = data[num_train + num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/hdd1/sams/data/avazu\"\n",
    "np.save(path + '/train.npy', train_data)\n",
    "np.save(path + '/val.npy', val_data)\n",
    "np.save(path + '/test.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6729485"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6729482"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6729485"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
